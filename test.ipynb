{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from glob import iglob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Bag:\n",
    "#     # Bag => WSI image cropped shuffled set\n",
    "#     # Instance => mixed_patches\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         excel_path=r\"patient-clinical-data.xlsx\",\n",
    "#         patches_path=r\"patches\",\n",
    "#         classification_label=\"ER\",\n",
    "#         positive_label=\"Positive\",\n",
    "#         bag_size=30, # Median of patches size is 36.5 FYI\n",
    "#     ) -> None:\n",
    "#         self.csv_data = pd.read_excel(excel_path)\n",
    "#         self.patch_folder = sorted(iglob(patches_path + \"/*\"))\n",
    "#         self.patch_label = list(\n",
    "#             map(\n",
    "#                 lambda x: 1 if x == positive_label else 0,\n",
    "#                 self.csv_data[classification_label],\n",
    "#             )\n",
    "#         )\n",
    "#         self.bag_size = bag_size\n",
    "#         print(\"Initialising\")\n",
    "#         print(\"labelling patch\")\n",
    "#         self.__patch_labelling()\n",
    "#         print(\"generating bags\")\n",
    "#         self.__gen_bags()\n",
    "#         print(\"finished generation\")\n",
    "#         self.bag_list = []\n",
    "#         self.bag_list.extend(zip(self.pos_bag_list, torch.ones(len(self.pos_bag_list))))\n",
    "#         self.bag_list.extend(zip(self.neg_bag_list, torch.zeros(len(self.neg_bag_list))))\n",
    "#         shuffle(self.bag_list)\n",
    "\n",
    "#     def __patch_labelling(self):\n",
    "#         self.unlabeled_img = []\n",
    "#         transform = transforms.Compose(\n",
    "#             [\n",
    "#                 transforms.Resize((256, 256)),\n",
    "#                 transforms.ToTensor(),\n",
    "#             ]\n",
    "#         )\n",
    "#         for folder in self.patch_folder:\n",
    "#             patch_img = []\n",
    "#             file_path = sorted(iglob(folder + \"/*\"))\n",
    "#             for img_path in file_path:\n",
    "#                 # ? Change it back to Pillow open and also pytorch transform and pytorch to_tensor()\n",
    "#                 patch_img.append(transform(Image.open(img_path)))\n",
    "#             self.unlabeled_img.append(patch_img)\n",
    "\n",
    "#         # self.labeled_img = []\n",
    "#         # for patch_idx, patch in enumerate(self.unlabeled_img):\n",
    "#         #     self.labeled_img.extend(zip(patch,list[self.patch_label[patch_idx]*len(patch)]))\n",
    "#         # self.pos_instance = []\n",
    "#         # self.neg_instance = []\n",
    "#         # for img,label in self.labeled_img:\n",
    "#         #     if label:\n",
    "#         #         self.pos_instance.append(img)\n",
    "#         #     else:\n",
    "#         #         self.neg_instance.append(img)\n",
    "\n",
    "#         self.pos_instance = []\n",
    "#         self.neg_instance = []\n",
    "#         for patch_idx, patch in enumerate(self.unlabeled_img):\n",
    "#             if self.patch_label[patch_idx]:\n",
    "#                 self.pos_instance.extend(patch)\n",
    "#             else:\n",
    "#                 self.neg_instance.extend(patch)\n",
    "#         ratio = len(self.pos_instance) / (\n",
    "#             len(self.pos_instance) + len(self.neg_instance)\n",
    "#         )\n",
    "#         self.pos_neg_ins_ratio = (ratio, ratio + 0.1)\n",
    "\n",
    "#     def __gen_bags(self):\n",
    "#         self.pos_bag_list = []\n",
    "#         self.neg_bag_list = []\n",
    "#         # Generate Positive Bags\n",
    "#         for i in range(np.count_nonzero(self.patch_label)):\n",
    "#             bag = []\n",
    "#             try:\n",
    "#                 bag_size = np.random.randint(*self.bag_size)\n",
    "#             except:\n",
    "#                 bag_size = self.bag_size\n",
    "#             pos_size = round(\n",
    "#                 (\n",
    "#                     np.random.random()\n",
    "#                     * (self.pos_neg_ins_ratio[1] - self.pos_neg_ins_ratio[0])\n",
    "#                     + self.pos_neg_ins_ratio[0]\n",
    "#                 )\n",
    "#                 * bag_size\n",
    "#             )\n",
    "#             for j in range(pos_size):\n",
    "#                 if self.pos_instance:\n",
    "#                     bag.append(self.pos_instance.pop(0)[0])\n",
    "#                 else:\n",
    "#                     break\n",
    "#             for j in range(bag_size - len(bag)):\n",
    "#                 if self.neg_instance:\n",
    "#                     bag.append(self.neg_instance.pop(0)[0])\n",
    "#                 else:\n",
    "#                     break\n",
    "#             self.pos_bag_list.append(bag)\n",
    "#         neg_index_list = []\n",
    "#         for i in range(len(self.patch_label) - np.count_nonzero(self.patch_label)):\n",
    "#             bag = []\n",
    "#             try:\n",
    "#                 bag_size = np.random.randint(*self.bag_size)\n",
    "#             except:\n",
    "#                 bag_size = self.bag_size\n",
    "#             for j in range(bag_size):\n",
    "#                 if self.neg_instance:\n",
    "#                     tmp_instance = self.neg_instance.pop(0)\n",
    "#                     bag.append(tmp_instance[0])\n",
    "#                     neg_index_list.append(tmp_instance[1])\n",
    "#                 else:\n",
    "#                     break\n",
    "#             self.neg_bag_list.append(bag)\n",
    "#         # print(neg_index_list)\n",
    "\n",
    "#         shuffle(self.pos_bag_list)\n",
    "#         shuffle(self.neg_bag_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag = Bag(\n",
    "#     excel_path=r\"patient-clinical-data.xlsx\",\n",
    "#     patches_path=r\"patches\",\n",
    "#     classification_label=\"ER\",\n",
    "#     positive_label=\"Positive\",\n",
    "#     bag_size=30,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bag_WSI:\n",
    "    # Bag => WSI image cropped shuffled set\n",
    "    # Instance => mixed_patches\n",
    "    def __init__(\n",
    "        self,\n",
    "        excel_path=r\"patient-clinical-data.xlsx\",\n",
    "        patches_path=r\"patches\",\n",
    "        classification_label=\"ER\",\n",
    "        positive_label=\"Positive\",\n",
    "        bag_size=30,  # Median of patches size is 36.5 FYI\n",
    "    ) -> None:\n",
    "        self.csv_data = pd.read_excel(excel_path)\n",
    "        self.patch_folder = sorted(\n",
    "            iglob(patches_path + \"/*\"), key=lambda x: int(x.split(\"/\")[-1])\n",
    "        )\n",
    "        self.patch_label = list(\n",
    "            map(\n",
    "                lambda x: 1 if x == positive_label else 0,\n",
    "                self.csv_data[classification_label],\n",
    "            )\n",
    "        )\n",
    "        self.bag_size = bag_size\n",
    "        print(\"Initialising\")\n",
    "        self.__patch_labelling()\n",
    "        print(\"finished generation\")\n",
    "        self.bag_list = list(\n",
    "            [\n",
    "                *zip(self.pos_bag_list, torch.ones(len(self.pos_bag_list),1)),\n",
    "                *zip(self.neg_bag_list, torch.zeros(len(self.neg_bag_list),1)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __patch_labelling(self):\n",
    "        unlabeled_img_list = []\n",
    "        self.pos_bag_list = []\n",
    "        self.neg_bag_list = []\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                # transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                # ? Need normalised the image?\n",
    "            ]\n",
    "        )\n",
    "        for folder in self.patch_folder:\n",
    "            patch_img = []\n",
    "            file_path = iglob(folder + \"/*\")\n",
    "            for img_path in file_path:\n",
    "                patch_img.append(transform(Image.open(img_path)))\n",
    "            unlabeled_img_list.append(torch.stack(patch_img, dim=0))\n",
    "        for patch_idx, patch in enumerate(unlabeled_img_list):\n",
    "            # * Check Patch Tensor Length\n",
    "            if len(patch) <= self.bag_size:\n",
    "                # * Generate a bag directly, can write a function to make it look better\n",
    "                if self.patch_label[patch_idx]:\n",
    "                    self.pos_bag_list.append(patch)\n",
    "                else:\n",
    "                    self.neg_bag_list.append(patch)\n",
    "            else:\n",
    "                # * Generate multiple bags with random sampled instances, number of bags depend on the number of instances the patch have.\n",
    "                bootstrap_times = len(patch) // self.bag_size + 1\n",
    "                for i in range(bootstrap_times):\n",
    "                    rand_int = torch.multinomial(\n",
    "                        torch.ones(self.bag_size, dtype=torch.float),\n",
    "                        self.bag_size,\n",
    "                        replacement=True,\n",
    "                    )\n",
    "                    bag = patch[rand_int]  # 1 * N(img) * 3 * 255 * 255\n",
    "                    if self.patch_label[patch_idx]:\n",
    "                        self.pos_bag_list.append(bag)\n",
    "                    else:\n",
    "                        self.neg_bag_list.append(bag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising\n",
      "finished generation\n"
     ]
    }
   ],
   "source": [
    "bag_wsi = Bag_WSI(\n",
    "    excel_path=r\"patient-clinical-data.xlsx\",\n",
    "    patches_path=r\"patches\",\n",
    "    classification_label=\"ER\",\n",
    "    positive_label=\"Positive\",\n",
    "    bag_size=30,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vgg](../BALNMP/imgs/vgg16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](../BALNMP/imgs/b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Feature extraction (left part of Figure 2b). N feature vectors were extracted for the N image instances in each bag by using a convolutional neural network (CNN) model.\n",
    "\n",
    "(3) MIL (right part of Figure 2b).The extracted N feature vectors of image instances were first processed by the max-pooling(already finished in vgg16_bn, please see the following [link](https://www.google.com/url?sa=i&url=https%3A%2F%2Fneurohive.io%2Fen%2Fpopular-networks%2Fvgg16%2F&psig=AOvVaw2Z-N_QJmi_T9Q3lVSHTYnl&ust=1665024222296000&source=images&cd=vfe&ved=0CAwQjRxqFwoTCICO_9yIyPoCFQAAAAAdAAAAABAD) for more information) **[Feng and Zhou, 2017, Pinheiro and Collobert, 2015, Zhu et al., 2017]** and reshaping and then were passed to a two-layer fully connected (FC) layer. The N weight factors for the instances in the bag were thus obtained and then were further multiplied to the original feature vectors [Ilse et al., 2018] to adaptively adjust the effect of instance features.Then, the fused features were fed into the classifier, and the outputs and the ground truth labels were used to calculate the cross-entropy loss. \n",
    "\n",
    "(4) Model training and testing. We randomly divided the WSIs into training cohort and independent test cohort with the ratio of 4:1 and randomly selected 25% of the training cohort as the validation cohort. We used Adam optimizer with learning rate 1e-4 to update the model parameters and weight decay 1e-3 for regularization. In the training phase, we used the cosine annealing warm restarts strategy to adjust the learning rate [Loshchilov and Hutter, 2017]. In the testing phase, the ALN status is predicted by aggregating the model outputs of all bags from the same slide (Figure 2c). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # Only use the CNN part and flatten part for vgg16_bn, without using the last classifier\n",
    "        vgg16_bn = torchvision.models.__dict__['vgg16_bn'](weights='DEFAULT')\n",
    "        self.extractor = nn.Sequential(*(list(vgg16_bn.children())[:-1]))\n",
    "        self.output_features_size = 512 * 7 * 7 # C * H * W\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # TODO: The vgg16_bn will autometically run on all different images?\n",
    "        feat = self.extractor(x)\n",
    "        # print('feat.shape = ',feat.shape) # N * 512 * 7 * 7\n",
    "        return feat\n",
    "    \n",
    "class FeatureAggregator(nn.Module):\n",
    "    def __init__(self,cnn_output_features_size) -> None:\n",
    "        super().__init__()\n",
    "        self.pink_size = 256\n",
    "        self.white_size = 128\n",
    "        # FC: 1*(512*7*7) -> 256\n",
    "        # Attention: 256 -> 1\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(cnn_output_features_size,self.pink_size),\n",
    "            nn.Dropout(),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        # I dont understand why the attention layer should write like this.\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.pink_size, self.white_size),\n",
    "            nn.Dropout(),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.white_size, 1),\n",
    "            nn.Dropout()\n",
    "            # No need another activation function as we will have softmax later in the forward part.\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1, 512*7*7) # Flatten into N Column(512*7*7)\n",
    "        x = self.fc(x) # N * (1 * 256)\n",
    "        \n",
    "        a = self.attention(x) #　N * 1\n",
    "        a = a.T #should be N*1 -> 1*N i think???\n",
    "        a = torch.softmax(a,dim=1)\n",
    "        \n",
    "        m = torch.mm(a , x) # 1*N x N*128 = 1*128\n",
    "        return m\n",
    "    \n",
    "class MIL(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(MIL, self).__init__()\n",
    "        # vgg16_bn without last classification layer\n",
    "        self.cnn = CNN()\n",
    "        self.attention = FeatureAggregator(self.cnn.output_features_size)\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            nn.Linear(self.attention.pink_size, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64,2),\n",
    "            nn.Sigmoid()\n",
    "            # Sigmoid?\n",
    "        )\n",
    "\n",
    "    def forward(self, bag):\n",
    "        feat = self.cnn(bag) # N * 3 * 256 * 256 -> N * 512 * 7 * 7\n",
    "        post_attention_feat = self.attention(feat)\n",
    "        x = self.fc(post_attention_feat)\n",
    "        # print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bag = len(bag_wsi.bag_list)\n",
    "train_bags, test_bags = torch.utils.data.random_split(\n",
    "    bag_wsi.bag_list, [round(0.01 * num_bag), round(0.99 * num_bag)]\n",
    ")\n",
    "train_patch, train_label = [i[0] for i in train_bags],[i[1] for i in train_bags]\n",
    "test_patch, test_label = [i[0] for i in test_bags],[i[1] for i in test_bags]\n",
    "\n",
    "mil = MIL()\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mil.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7414, grad_fn=<NllLossBackward0>) tensor([[0.0118, 0.1061]], grad_fn=<AddmmBackward0>) tensor([0.])\n",
      "tensor(2.4978, grad_fn=<NllLossBackward0>) tensor([[ 1.7371, -0.6749]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(0.6506, grad_fn=<NllLossBackward0>) tensor([[0.0680, 0.1549]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(0.4960, grad_fn=<NllLossBackward0>) tensor([[-0.0854,  0.3575]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(0.0908, grad_fn=<NllLossBackward0>) tensor([[-1.0005,  1.3533]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward0>) tensor([[-4.5685,  5.1401]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(17.9399, grad_fn=<NllLossBackward0>) tensor([[-8.2897,  9.6502]], grad_fn=<AddmmBackward0>) tensor([0.])\n",
      "tensor(0.0016, grad_fn=<NllLossBackward0>) tensor([[-3.0633,  3.3722]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(0.0037, grad_fn=<NllLossBackward0>) tensor([[-1.9154,  3.6935]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(0.0865, grad_fn=<NllLossBackward0>) tensor([[-1.0322,  1.3717]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(3.9607, grad_fn=<NllLossBackward0>) tensor([[-1.4480,  2.4935]], grad_fn=<AddmmBackward0>) tensor([0.])\n",
      "tensor(0.1484, grad_fn=<NllLossBackward0>) tensor([[-0.7592,  1.0739]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(0.1719, grad_fn=<NllLossBackward0>) tensor([[-0.1944,  1.4792]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(0.0090, grad_fn=<NllLossBackward0>) tensor([[-1.5210,  3.1847]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(0.2691, grad_fn=<NllLossBackward0>) tensor([[-0.2080,  0.9673]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(1.1908, grad_fn=<NllLossBackward0>) tensor([[0.1300, 0.9585]], grad_fn=<AddmmBackward0>) tensor([0.])\n",
      "tensor(0.3017, grad_fn=<NllLossBackward0>) tensor([[-0.2425,  0.8013]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(0.0401, grad_fn=<NllLossBackward0>) tensor([[-1.0486,  2.1474]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(0.0344, grad_fn=<NllLossBackward0>) tensor([[0.2691, 3.6214]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "[1,    20] loss: 0.014\n",
      "tensor(0.0438, grad_fn=<NllLossBackward0>) tensor([[-0.5595,  2.5470]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(0.0014, grad_fn=<NllLossBackward0>) tensor([[-1.2154,  5.3301]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(0.2888, grad_fn=<NllLossBackward0>) tensor([[-0.4574,  0.6366]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(0.0458, grad_fn=<NllLossBackward0>) tensor([[-1.0093,  2.0514]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(1.3255, grad_fn=<NllLossBackward0>) tensor([[-0.1949,  0.8218]], grad_fn=<AddmmBackward0>) tensor([0.])\n",
      "tensor(0.0248, grad_fn=<NllLossBackward0>) tensor([[-0.2057,  3.4803]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(3.4759, grad_fn=<NllLossBackward0>) tensor([[-1.6423,  1.8022]], grad_fn=<AddmmBackward0>) tensor([0.])\n",
      "tensor(0.0265, grad_fn=<NllLossBackward0>) tensor([[-0.0854,  3.5317]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(0.0428, grad_fn=<NllLossBackward0>) tensor([[-0.7590,  2.3701]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(0.0518, grad_fn=<NllLossBackward0>) tensor([[0.1309, 3.0658]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "tensor(1.7115, grad_fn=<NllLossBackward0>) tensor([[0.3015, 1.8138]], grad_fn=<AddmmBackward0>) tensor([0.])\n",
      "tensor(1.8789, grad_fn=<NllLossBackward0>) tensor([[-0.3783,  1.3349]], grad_fn=<AddmmBackward0>) tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    for patch, label in train_bags:\n",
    "        count += 1\n",
    "        optimizer.zero_grad()\n",
    "        # print('patch length: ',len(patch))\n",
    "        bag_class = mil(patch)\n",
    "        loss = loss_func(bag_class,label.to(torch.long))\n",
    "        print(loss,bag_class,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if count % 20 == 19:\n",
    "            print(f\"[{epoch + 1}, {count + 1:5d}] loss: {running_loss / 2000:.3f}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('image')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddd80f1155ab6ac1b3603eec8a4f7af7e571d6991b6f6e5941db98bef78140f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
